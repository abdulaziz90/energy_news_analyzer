{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Assessing Potential Impact of Energy News\n",
    "\n",
    "This notebook demonstrates the experimental impact assessment feature using the `analysis` service.\n",
    "\n",
    "**Objective:** Use a simple LLM chain (via Langchain and a Hugging Face model like GPT-2) to provide a qualitative assessment of the potential impact of a news article on a specified context (e.g., the energy market, oil prices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** The impact assessment provided by the current basic model (`gpt2`) is experimental and likely simplistic. A more sophisticated assessment would require a larger, more capable LLM and potentially more complex prompting or a dedicated reasoning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import libraries, add the project root to the path, and load the analysis service. This will load the LLM pipeline specified in `analysis.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import the analysis service\n",
    "try:\n",
    "    from api.services import analysis\n",
    "    import importlib\n",
    "    importlib.reload(analysis) # Ensure latest code and models are loaded\n",
    "    models_loaded = analysis.impact_llm is not None\n",
    "    if models_loaded:\n",
    "        logger.info(\"Impact assessment LLM pipeline loaded successfully.\")\n",
    "    else:\n",
    "        logger.warning(\"Impact assessment LLM pipeline failed to load. Check logs in analysis.py.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to import or load analysis service: {e}\", exc_info=True)\n",
    "    models_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Summaries (or Original Articles)\n",
    "\n",
    "We can use either the full article content or the summaries generated in the previous notebook as input for the impact assessment. Summaries are generally better as they provide concise information to the LLM. We'll load the summaries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "summaries_file = os.path.join("data", "summary_results.json")\n",
    "articles = [] # Also load original articles for fallback
    "articles_file = os.path.join("data", "fetched_articles.json")

    "if os.path.exists(summaries_file):\n",
    "    try:\n",
    "        with open(summaries_file, 'r') as f:\n",
    "            summaries = json.load(f)\n",
    "        print(f"Loaded {len(summaries)} summaries from {summaries_file}")\n",
    "    except Exception as e:\n",
    "        print(f"Error loading summaries from {summaries_file}: {e}")
    "else:\n",
    "    print(f"Warning: {summaries_file} not found. Will try to use original articles.")

    "if not summaries:
    "    if os.path.exists(articles_file):
    "        try:
    "            with open(articles_file, 'r') as f:
    "                articles = json.load(f)
    "            print(f"Loaded {len(articles)} original articles from {articles_file}")
    "        except Exception as e:
    "            print(f"Error loading articles from {articles_file}: {e}")
    "    else:
    "       print(f"Error: Neither summaries nor original articles found. Please run Notebook 1 and 3 first.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Impact Assessment\n",
    "\n",
    "Iterate through the summaries (or articles) and apply the `assess_impact` function. We need to provide a context for the assessment (e.g., 'energy market', 'oil prices', 'renewable energy stocks')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_loaded and (summaries or articles):\n",
    "    print("Performing impact assessment...")\n",
    "    results = []\n",
    "    # Determine the input source (summaries preferred)
    "    input_data = summaries if summaries else articles
    "    context = "global energy market" # Define a general context
    "    
    "    for i, item in enumerate(input_data):\n",
    "        title = item.get('title', 'No Title')
    "        # Get text: summary if available, else content/description from original article
    "        text_to_assess = None
    "        if 'summary' in item and item['summary']:
    "            text_to_assess = item['summary']
    "        elif 'content' in item and item['content']:
    "             text_to_assess = item['content']
    "        elif 'description' in item and item['description']:
    "             text_to_assess = item['description']

    "        print(f"\nAssessing impact for article {i+1}: {title}")
    "        print(f"Context: {context}")
    "        
    "        if text_to_assess:
    "            impact_assessment = analysis.assess_impact(text_to_assess, context=context)
    "            if impact_assessment:
    "                print(f"  Potential Impact: {impact_assessment}")
    "                results.append({
    "                    'title': title,
    "                    'url': item.get('url'),
    "                    'context': context,
    "                    'assessment': impact_assessment
    "                })
    "            else:\n",
    "                print("  Impact assessment failed for this article.")
    "                results.append({
    "                    'title': title,
    "                    'url': item.get('url'),
    "                    'error': 'Impact assessment failed'
    "                })
    "        else:\n",
    "            print("  Skipping assessment: No summary or content available.")
    "            results.append({
    "                'title': title,
    "                'url': item.get('url'),
    "                'error': 'No text available for assessment'
    "            })
    "            
    "    # Save results
    "    results_file = os.path.join("data", "impact_assessment_results.json")
    "    with open(results_file, 'w') as f:
    "        json.dump(results, f, indent=2)
    "    print(f"\nSaved impact assessment results to {results_file}")
    "            
    "elif not models_loaded:
    "    print("Skipping impact assessment as the LLM pipeline failed to load.")
    "elif not summaries and not articles:
    "    print("Skipping impact assessment as no summaries or articles were loaded.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "This notebook demonstrated the experimental impact assessment using a basic LLM chain provided by the `analysis.py` service. The qualitative assessments for the specified context have been saved to `notebooks/data/impact_assessment_results.json`. Remember that the quality of this assessment is highly dependent on the underlying LLM used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

